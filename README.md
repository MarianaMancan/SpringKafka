üì¶ Kafka Producer/Consumer com Docker

Este projeto demonstra uma arquitetura b√°sica de streaming de dados com Apache Kafka, utilizando Docker Compose para orquestra√ß√£o dos containers Kafka e Zookeeper.

üß± Arquitetura

A imagem abaixo representa a arquitetura usada no projeto:
![image](https://github.com/user-attachments/assets/575b5a07-a5d3-4bc9-9ca9-9f913ebaf0f2)

---Producer envia mensagens (streams de registros) para um t√≥pico Kafka.

---As mensagens s√£o armazenadas em um log particionado e replicado.

---Consumer consome as mensagens do t√≥pico.

Um t√≥pico no Kafka √© uma cole√ß√£o de eventos que pode ser:

---Replicado e particionado

--Dur√°vel (por horas, dias ou anos)

--Grande ou pequeno

‚ú® Como executar o projeto

‚úÖ 1. Subir os containers
docker-compose up -d
Esse comando ir√° subir os servi√ßos zookeeper e kafka com as configura√ß√µes apropriadas.

üîç 2. Listar os t√≥picos dispon√≠veis no broker
docker exec -it kafka kafka-topics --bootstrap-server localhost:29092 --list
O Kafka est√° configurado com localhost:29092 para acesso externo via Docker.

 3. Consumir mensagens de um t√≥pico
 4. docker exec -it kafka kafka-console-consumer \
  --topic mensagem \
  --from-beginning \
  --bootstrap-server kafka:9092

Este comando consome todas as mensagens desde o in√≠cio do t√≥pico mensagem.

üõ† Tecnologias usadas

Docker

Docker Compose

Apache Kafka 7.6.0

Zookeeper 7.6.0

Spring Boot (como exemplo de aplica√ß√£o produtora ou consumidora)
